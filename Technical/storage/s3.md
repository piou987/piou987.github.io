---
sort: 2
---

# S3

## Reference

| **URL**                                              | **Description**   |
| :--------------------------------------------------- | :---------------- |
| https://docs.aws.amazon.com/ko_kr/AmazonS3/          | AWS Docs          |
| https://docs.aws.amazon.com/cli/latest/reference/s3/ | AWS CLI Reference |

S3는 Docs만 봐도 충분히 이해할 수 있었습니다.

## Basic Concept

쉽게!! 스토리지에 접근할 수 있도록 제공해주기 위해서 만들어 졌다고 한다. API에서도 사용하기 쉽고, 업로드도 쉽고, 다운로드도 쉽다고 한다. 가장 근본적인 Concept을 이해하고 있자.

> Amazon Simple Storage Service는 인터넷용 스토리지 서비스입니다. 이 서비스는 개발자가 더 쉽게 웹 규모 컴퓨팅 작업을 수행할 수 있도록 설계되었습니다.
>
> Amazon S3에서 제공하는 단순한 웹 서비스 인터페이스를 사용하여 웹에서 언제 어디서나 원하는 양의 데이터를 저장하고 검색할 수 있습니다. 또한 개발자는 Amazon이 자체 웹 사이트의 글로벌 네트워크 운영에 사용하는 것과 같은 높은 확장성과 신뢰성을 갖춘 빠르고 경제적인 데이터 스토리지 인프라에 액세스할 수 있습니다. 이 서비스의 목적은 규모의 이점을 극대화하고 개발자들에게 이러한 이점을 제공하는 것입니다.

 

## Bucket, Object

물론 AWS Docs에 자세하게 나와 있는 부분이다. 조금 헷갈릴 수 있는 부분이나, 중요한 부분만 기술하겠다.

1. S3는 Object Storage이다. Bucket과 Object로만 이루어져 있다. 헷갈릴 수도 있는 부분은 S3에서의 Folder이다. S3에서 Folder는 File System에서처럼 계층 구조를 가지지 않는다. 단순히 논리적으로 구분짓기 위해 사용되는 것 뿐이고, 이후 Partitionning에 사용되기도 한다.
2. Bucket은 해당 Region의 모든 aws계정에서 유일하다. 즉, 내 계정에서만 유일한 것이 아니다. 이 점을 반드시 알고 있어야 당황하지 않는다.
3. 모든 정책은 Bucket단위로 결정된다. 물론 특정 folder하위를 가리키는 arn을 활용하여 bucket policy를 작성할 수 있지만 이 또한 Bucket에 적용하는 정책이다.

 

## Storage Class

개인적으로 S3에서 가장 중요한 부분이라고 생각한다.

S3에서는 Class를 나눠 놓고, 저장되는 Object가 사용되는 특성에 맞게 cost-effective하게 사용할 수 있도록 해두었다. 그리고 이런 정책을 더 적극적으로 활용하기 위한 lifecycle이 있다. 이 또한 뒤에서 살펴볼 것이다.

| **스토리지 클래스**     | **다음으로 설계됨**                                          | **내구성(설계상)** | **가용성(설계상)**   | **가용 영역** | **최소 스토리지 기간** | **최소 요금 객체 크기** | **기타 고려 사항**                                           |
| :---------------------- | :----------------------------------------------------------- | :----------------- | :------------------- | :------------ | :--------------------- | :---------------------- | :----------------------------------------------------------- |
| S3 Standard             | 자주 액세스하는 데이터                                       | 99.999999999%      | 99.99%               | >= 3          | 없음                   | 없음                    | 없음                                                         |
| S3 Standard-IA          | 수명이 길고 자주 액세스하지 않는 데이터                      | 99.999999999%      | 99.9%                | >= 3          | 30일                   | 128KB                   | GB당 검색 요금이 적용됩니다.                                 |
| S3 Intelligent-Tiering  | 변경 또는 알 수 없는 액세스 패턴으로 수명이 긴 데이터        | 99.999999999%      | 99.9%                | >= 3          | 30일                   | 없음                    | 객체당 모니터링 및 자동화 비용이 적용됩니다. 검색 요금이 없습니다. |
| S3 One Zone-IA          | 수명이 긴 데이터에 자주 액세스하지 않는 중요하지 않은 데이터 | 99.999999999%      | 99.5%                | 1             | 30일                   | 128KB                   | GB당 검색 요금이 적용됩니다. 가용 영역의 손실에 대한 복원력이 없습니다. |
| S3 Glacier              | 분에서 시간 단위로 검색 시간을 지원하는 장기간 데이터 보관   | 99.999999999%      | 99.99%(객체 복원 후) | >= 3          | 90일                   | 40KB                    | GB당 검색 요금이 적용됩니다. 이 객체에 액세스하려면 먼저 보관된 객체를 복원해야 합니다. 자세한 내용은 [보관된 객체의 복원](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/restoring-objects.html) 단원을 참조하십시오. |
| S3 Glacier Deep Archive | 12시간의 기본 검색 시간으로 거의 액세스하지 않는 데이터 아카이빙 | 99.999999999%      | 99.99%(객체 복원 후) | >= 3          | 180일                  | 40KB                    | GB당 검색 요금이 적용됩니다. 이 객체에 액세스하려면 먼저 보관된 객체를 복원해야 합니다. 자세한 내용은 [보관된 객체의 복원](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/restoring-objects.html) 단원을 참조하십시오. |
| RRS(권장되지 않음)      | 자주 액세스하는 중요하지 않은 데이터                         | 99.99%             | 99.99%               | >= 3          | 없음                   | 없음                    | 없음                                                         |

 

## lifecycle

개념은 굉장히 간단하다, 특성 기간이 지나면 자동으로 Class를 변경해주거나 expire(삭제) 해준다.

expire policy와 translation policy가 있다.

각 Class별 최소 기간이 있어서 동작을 테스트 해보려면 30일이 넘게 걸리는 정책도 있다.

나는 간단하게 하루만에 동작을 확인할 수 있는 변환 정책과 만료 정책을 테스트해보았다.

> 실습 시 주의할 부분은 128KB이하의 데이터는 Lifecycle정책이 먹지 않는다는 것이다.
>
> sample data file을 10MB ~ 100MB정도 잡고 테스트 해보아야한다.

![                         Amazon S3 스토리지 클래스 폭포형 그래픽.                     ](https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/images/SupportedTransitionsWaterfallModel.png)

또한, 클래스 간 전환 시 위와 같은 규칙을 가진다. 공식 문서에서는 이를 waterfall model이라고 한다.

 

## Bucket policy

https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/example-bucket-policies.html 

예제들도 세세하게 나와있다. 요약하자면, 사용자 기반으로 정책을 수립할 수도 있고, IP대역으로도 할 수 있고, Cloudfront OAI를 기반으로도 할 수 있다. (Cloudfront OAI의 대한 자세한 개념은 설명을 생략하겠다.) 심지어는 MFA인증까지 껴넣을 수 있다.

 

## Static web hosting

https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/HowDoIWebsiteConfiguration.html 

간단하다. 단순히 S3 bucket을 web-hosting용으로 등록하고, index.html파일을 작성하여 업로드 하면된다. 주의할 점은 public access를 허용해줘야 한다는 것이다. 그렇게 된다면 어디서든 접속할 수 있게 된다. 여기서 bucket policy를 잘 작성해야 한다. 특정 IP에서만 접근할 수 있도록 할 수 있을 것이다.

만약 앞단에 Cloudfront를 두게 된다면 주의해야할 부분이 있다.

1. **버킷의 객체는 AWS KMS로 암호화할 수 없음**

   CloudFront 배포는 AWS KMS로 암호화된 객체를 지원하지 않습니다. 배포를 사용하여 서비스하려는 S3 객체에서 KMS 암호화를 제거해야 합니다.

2. S3 를 단순히 web hosting용으로 사용하고자 할 때와 REST API로 사용하고자 할 때 Cloudfront 배포 시 **Origin 지정**에 주의해야 한다.

   web-hosting : [**AWSDOC-EXAMPLE-BUCKET.s3-website-region.amazonaws.com**](http://awsdoc-example-bucket.s3-website-region.amazonaws.com/)

   REST API : [**AWSDOC-EXAMPLE-BUCKET.s3.amazonaws.com**](http://awsdoc-example-bucket.s3.amazonaws.com/)

3. **Bucket Policy**에 주의한다. web hosting의 경우 Get:Object권한이 필요할 것이고 REST API의 경우는 더 많은 권한이 요구될 경우도 있을 것 이다.

403 error에 대처하는 방안에 대한 사이트이다.

https://aws.amazon.com/ko/premiumsupport/knowledge-center/cloudfront-serve-static-website/

이것도 꽤나 도움이 될 듯하다.

https://aws.amazon.com/ko/premiumsupport/knowledge-center/s3-cloudfront-website-access/

 

S3 static web hosting에 Cloudfront를 앞단에 붙이고나서, Cloudfront URL을 통해서만 해당 site에 접근하도록 유도하고 싶다면, Cloudfront의 OAI를 이용하여 S3 Bucket policy를 작성하는 방법이있다.

**OAI (Origin Access Identifier)**는 Cloud front가 Origin에 access할 때 가지는 고유한 ID 값이다. 이를 Bucket policy의 Principle의 값으로 사용할 수 있다.

https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html

S3로의 직접적인 접근은 막고 Cloudfront으로만 접속을 하게끔한다. 그렇게 해서 가지는 이점은, Cloudfront에서 Signed URL, Cookie를 통해 인증을 수행할 수도 있고, S3 static web hosting에서 지원하지 않는 HTTPS통신을 유도할 수도 있다.

 

Cloudfront HTTPS for S3

중요 포인트는 Route53 record를 추가하는 것인데, bucket name이 www.skill53.cloud 처럼 URL형태여야 한다.

https://karelledru.com/2016/06/static-site-hosting-on-S3-and-CloudFront/

 

## AWS-CLI

### mb

Bucket을 생성한다. (make bucket의 줄임말이다.) 여기서는 region만 주의해주면 된다.

```
1 2 ``$ aws s3 mb s3://worldskills2021-sample-bucket --region ap-northeast-1 make_bucket: worldskills2021-sample-bucket
```

 

### ls

list 조회 시 사용한다.

1. bucket 조회

   `1 2 3 4 5 6 7 8 ``$ aws s3 ls 2020-08-04 16:15:12 worldskills2021-artifact 2020-08-07 17:24:31 worldskills2021-athena-output 2020-08-04 09:59:16 worldskills2021-cloudformation-template 2020-08-10 16:08:25 worldskills2021-log 2020-08-10 10:01:21 worldskills2021-s3-sample 2020-08-10 18:54:51 worldskills2021-sample-bucket 2020-08-10 16:30:37 www.skill53.cloud`

    

2. 특정 위치만 조회

   `1 2 3 4 5 ``$ aws s3 ls s3://worldskills2021-s3-sample              PRE Standard-IA/              PRE Standard/              PRE expire/              PRE glacier/`

    

3. --recursive

   `1 2 3 4 5 6 7 8 9 10 11 12 ``$ aws s3 ls s3://worldskills2021-s3-sample --recursive 2020-08-08 16:23:16      0 Standard-IA/ 2020-08-08 16:37:02     11 Standard-IA/sample.txt 2020-08-10 10:14:53  104857600 Standard-IA/sample_file 2020-08-08 16:22:56      0 Standard/ 2020-08-08 16:37:41     11 Standard/sample.txt 2020-08-10 10:14:32  104857600 Standard/sample_file 2020-08-10 10:13:59     11 expire/sample.txt 2020-08-10 10:13:29  104857600 expire/sample_file 2020-08-10 17:05:21      0 glacier/ 2020-08-10 17:05:55  104857600 glacier/sample_file 2020-08-10 17:06:21  104857600 glacier/sample_file_2`

    

4. --summarize

   `1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ``$ aws s3 ls s3://worldskills2021-s3-sample --recursive --summarize 2020-08-08 16:23:16      0 Standard-IA/ 2020-08-08 16:37:02     11 Standard-IA/sample.txt 2020-08-10 10:14:53  104857600 Standard-IA/sample_file 2020-08-08 16:22:56      0 Standard/ 2020-08-08 16:37:41     11 Standard/sample.txt 2020-08-10 10:14:32  104857600 Standard/sample_file 2020-08-10 10:13:59     11 expire/sample.txt 2020-08-10 10:13:29  104857600 expire/sample_file 2020-08-10 17:05:21      0 glacier/ 2020-08-10 17:05:55  104857600 glacier/sample_file 2020-08-10 17:06:21  104857600 glacier/sample_file_2  Total Objects: 11  Total Size: 524288033`

    

5. --human-readable

   `1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ``$ aws s3 ls s3://worldskills2021-s3-sample --recursive --summarize --human-readable 2020-08-08 16:23:16   0 Bytes Standard-IA/ 2020-08-08 16:37:02  11 Bytes Standard-IA/sample.txt 2020-08-10 10:14:53  100.0 MiB Standard-IA/sample_file 2020-08-08 16:22:56   0 Bytes Standard/ 2020-08-08 16:37:41  11 Bytes Standard/sample.txt 2020-08-10 10:14:32  100.0 MiB Standard/sample_file 2020-08-10 10:13:59  11 Bytes expire/sample.txt 2020-08-10 10:13:29  100.0 MiB expire/sample_file 2020-08-10 17:05:21   0 Bytes glacier/ 2020-08-10 17:05:55  100.0 MiB glacier/sample_file 2020-08-10 17:06:21  100.0 MiB glacier/sample_file_2  Total Objects: 11  Total Size: 500.0 MiB`

 

### cp

3가지의 copy가 해용~ 그냥 어디서 어디로 옮길 수 있는 지 알아둬여~

- s3 → local
- local → s3
- s3 → s3

기본적인 커맨드는 아래와 같다.

```
1 2 ``$ aws s3 cp sample.txt s3://worldskills2021-sample-bucket/ upload: ./sample.txt to s3://worldskills2021-sample-bucket/sample.txt
```

 

- -recursive

디렉터리 하위를 전부~! 복사한다~

```
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ``$ aws s3 cp sample/ s3://worldskills2021-sample-bucket/ --recursive upload: sample/sample.txt to s3://worldskills2021-sample-bucket/sample.txt upload: sample/sample2.txt to s3://worldskills2021-sample-bucket/sample2.txt upload: sample/sample.zip to s3://worldskills2021-sample-bucket/sample.zip upload: sample/sample2.zip to s3://worldskills2021-sample-bucket/sample2.zip upload: sample/sample3.zip to s3://worldskills2021-sample-bucket/sample3.zip upload: sample/sample3.txt to s3://worldskills2021-sample-bucket/sample3.txt upload: sample/sample4.txt to s3://worldskills2021-sample-bucket/sample4.txt upload: sample/sample4.zip to s3://worldskills2021-sample-bucket/sample4.zip upload: sample/sample.dump to s3://worldskills2021-sample-bucket/sample.dump upload: sample/sample5.txt to s3://worldskills2021-sample-bucket/sample5.txt upload: sample/sample5.zip to s3://worldskills2021-sample-bucket/sample5.zip upload: sample/sample2.dump to s3://worldskills2021-sample-bucket/sample2.dump upload: sample/sample5.dump to s3://worldskills2021-sample-bucket/sample5.dump upload: sample/sample4.dump to s3://worldskills2021-sample-bucket/sample4.dump upload: sample/sample3.dump to s3://worldskills2021-sample-bucket/sample3.dump
```

 

- -include, --exclude

정규표현식을 활용해서 include, exclude할 수 있다~!

```
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ``$ aws s3 cp sample/ s3://worldskills2021-sample-bucket/ --recursive --include '*.txt' upload: sample/sample.txt to s3://worldskills2021-sample-bucket/sample.txt upload: sample/sample2.txt to s3://worldskills2021-sample-bucket/sample2.txt upload: sample/sample.zip to s3://worldskills2021-sample-bucket/sample.zip upload: sample/sample2.zip to s3://worldskills2021-sample-bucket/sample2.zip upload: sample/sample3.txt to s3://worldskills2021-sample-bucket/sample3.txt upload: sample/sample3.zip to s3://worldskills2021-sample-bucket/sample3.zip upload: sample/sample4.txt to s3://worldskills2021-sample-bucket/sample4.txt upload: sample/sample4.zip to s3://worldskills2021-sample-bucket/sample4.zip upload: sample/sample5.txt to s3://worldskills2021-sample-bucket/sample5.txt upload: sample/sample.dump to s3://worldskills2021-sample-bucket/sample.dump upload: sample/sample5.zip to s3://worldskills2021-sample-bucket/sample5.zip upload: sample/sample4.dump to s3://worldskills2021-sample-bucket/sample4.dump upload: sample/sample3.dump to s3://worldskills2021-sample-bucket/sample3.dump upload: sample/sample5.dump to s3://worldskills2021-sample-bucket/sample5.dump upload: sample/sample2.dump to s3://worldskills2021-sample-bucket/sample2.dump
```

 

ㅇ엥ㅇ?ㅇ? .txt 확장자만 복사되어야 하는 거 아니에용???

아~!! exclude도 같이 조합해서 써야하는 구낭~! 그럼 나머지는 전부 제외한다는 의미에서 아래처럼 하면 되겠당~!@#!

```
1 2 ``$ aws s3 cp sample/ s3://worldskills2021-sample-bucket/ --recursive --include '*.txt' --exclude '*' 
```

 

해치웠나...?

(2초 뒤...) ㅇㅁㄴㅇㅋㅌㅊ?ㅃㅉㄸ? 이번엔 아예 복사가 안된다구?!?!?!

젠장... 만만치 않은 녀석이군...! exclude를 include보다 먼저 작성해줘야 하다니...! 크ㅡㅎㅂ...!!

```
1 2 3 4 5 6 ``$ aws s3 cp sample/ s3://worldskills2021-sample-bucket/ --recursive --exclude '*' --include '*.txt' upload: sample/sample.txt to s3://worldskills2021-sample-bucket/sample.txt upload: sample/sample5.txt to s3://worldskills2021-sample-bucket/sample5.txt upload: sample/sample3.txt to s3://worldskills2021-sample-bucket/sample3.txt upload: sample/sample4.txt to s3://worldskills2021-sample-bucket/sample4.txt upload: sample/sample2.txt to s3://worldskills2021-sample-bucket/sample2.txt
```

 

optional을 조금 살펴보자

1. --dryrun (실제 실행하지 않고 실행하고자 하는 바를 display해준다.)

   `1 2 ``$ aws s3 cp sample.txt s3://worldskills2021-sample-bucket/ --dryrun (dryrun) upload: ./sample.txt to s3://worldskills2021-sample-bucket/sample.txt`

    

2. --quiet (조용히~!)

   `1 2 ``$ aws s3 cp sample.txt s3://worldskills2021-sample-bucket/ --quiet `

    

3. --storage-class (class를 고를 수 있는 거겠죵?)

   `1 2 ``$ aws s3 cp sample/sample.txt s3://worldskills2021-sample-bucket/ --storage-class STANDARD_IA upload: sample/sample.txt to s3://worldskills2021-sample-bucket/sample.txt`

    

4. --expires (지정한 시간에 삭제되도록 할 수 있넹~)

   `1 2 ``$ aws s3 cp sample/sample2.txt s3://worldskills2021-sample-bucket/ --storage-class STANDARD_IA --expires '2020.08.10 20:00:00' upload: sample/sample2.txt to s3://worldskills2021-sample-bucket/sample2.txt`

    

### sync

cp랑 똑같아요~ 근데 조금 다른건 이녀석은 비교해서 수정된 녀석들만 copy한답니다~!@

 

### sync performance issue

sync를 통해 데이터 전송을하고 있다면, 아래 페이지에서 설명하는 문제에 고착될 수 있다. 한번 보고 넘어가자.

요약하자면, sync는 소스 파일과 대상 파일을 비교하여 수정된 소스만 전송을 하는 데, 파일 크기가 크고, 파일 수가 많다면 비교하는 연산이 많아져서 속도가 느려질 수 있다는 이야기이다.

https://aws.amazon.com/ko/premiumsupport/knowledge-center/s3-improve-transfer-sync-command

 

### mv

move~! 옮겨준다는 거겠죠? 이름 처럼 매우 간단하고, cp와 옵션이 똑같아요~

 

### rb

bucket에 뭐가 있든 비우고 지워버리긔~!

```
1 ``$ aws s3 rb s3://bucket-name --force  
```